# Justification des choix technologiques – MSPR COVID & MPOX

Dans le cadre de ce projet, chaque choix technologique a été mûrement réfléchi afin de répondre aux exigences d’un pipeline Data/IA moderne, robuste et maintenable. Voici, en détail, les raisons qui ont guidé la sélection de chaque composant majeur de l’architecture.

Pour la base de données, j’ai choisi PostgreSQL. Ce système est reconnu pour sa robustesse, sa fiabilité et sa gestion avancée des transactions, ce qui est essentiel pour garantir l’intégrité des données, notamment dans le domaine de la santé. PostgreSQL offre une grande richesse fonctionnelle : il gère parfaitement les types avancés comme les dates, les grands entiers (BigInt), ou encore le JSON, et permet de créer des index et des jointures performantes, ce qui est indispensable pour l’analytique. Son caractère open source, sa gratuité et la taille de sa communauté en font un choix sûr et pérenne. Il s’intègre très bien avec les outils utilisés dans ce projet, que ce soit Prisma côté Node.js ou les bibliothèques Python pour l’ETL. Enfin, il est capable de gérer de gros volumes de données, ce qui était nécessaire ici, et propose des mécanismes de sécurité adaptés aux contraintes RGPD.

Pour la partie ETL et data science, Python s’est imposé naturellement. Son écosystème est extrêmement riche : des bibliothèques comme Pandas ou NumPy facilitent la manipulation et le nettoyage des données, tandis que d’autres comme Pandas Profiling, Matplotlib ou Seaborn permettent d’analyser et de visualiser rapidement la qualité des jeux de données. Python est aussi très apprécié pour sa rapidité de prototypage et la clarté de sa syntaxe, ce qui permet de développer des scripts efficaces et faciles à maintenir. Il s’interface sans difficulté avec PostgreSQL, que ce soit pour l’extraction ou le chargement de données.

Pour le chargement massif et l’orchestration, j’ai opté pour Node.js associé à Prisma. Prisma permet de modéliser la base de données de façon déclarative, de générer automatiquement le schéma, et de sécuriser les accès aux données. Node.js, de son côté, est parfaitement adapté à la gestion asynchrone des opérations lourdes, comme le chargement de gros volumes de données. Ce choix facilite aussi la communication entre l’ETL Python et l’API Express, tout en assurant de bonnes performances et une grande fiabilité.

L’API REST a été développée avec Express.js, le framework Node.js le plus utilisé pour ce type d’application. Express est simple, modulaire, et dispose d’un vaste écosystème de middlewares pour la sécurité, la gestion des erreurs ou la documentation. Il permet de structurer le code de façon claire et maintenable, et offre des performances largement suffisantes pour exposer des données analytiques. L’intégration de Swagger/OpenAPI a été un atout supplémentaire, car elle permet de générer automatiquement une documentation interactive, facilitant l’intégration et les tests pour tout utilisateur ou développeur tiers.

Enfin, la documentation et la traçabilité ont été assurées grâce à l’utilisation du format Markdown pour tous les guides, comptes rendus et documents techniques, ce qui garantit une lecture facile et une bonne intégration sur GitHub. L’utilisation de Swagger/OpenAPI pour l’API permet de maintenir une documentation toujours à jour et interactive, tandis que la génération de logs à chaque étape du pipeline assure la traçabilité et le contrôle qualité du projet.

En résumé, chaque technologie a été choisie pour sa robustesse, sa compatibilité avec les autres briques du projet, sa facilité de maintenance et sa conformité avec les standards professionnels du secteur Data/IA. Cette cohérence technologique garantit la réussite et la pérennité du pipeline mis en place. 